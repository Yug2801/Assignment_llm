{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object DataFrame.iterrows at 0x00000197384CFA50>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "base_url = \"https://stanford-cs324.github.io/winter2022/lectures/\"\n",
    "\n",
    "def get_title_from_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    path_components = parsed_url.path.split('/')\n",
    "    if len(path_components) >= 3:\n",
    "        return path_components[-2]\n",
    "    return None\n",
    "\n",
    "def scrape_lecture_notes(base_url):\n",
    "    try:\n",
    "        lecture_notes = []\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        main_content_div = soup.find('div', id='main-content')\n",
    "\n",
    "        if main_content_div:\n",
    "            lecture_links = main_content_div.find_all('a', href=True)\n",
    "\n",
    "            output_folder = r\"E:\\Assignment2\\data\\lecture_notes\"\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "            existing_files = glob.glob(os.path.join(output_folder, '*.txt'))\n",
    "            for file in existing_files:\n",
    "                os.remove(file)\n",
    "\n",
    "            for idx, link in enumerate(lecture_links, start=1):\n",
    "                lecture_url = urljoin(base_url, link['href'])\n",
    "                lecture_title = get_title_from_url(lecture_url)\n",
    "\n",
    "                lecture_response = requests.get(lecture_url)\n",
    "                lecture_response.raise_for_status()\n",
    "\n",
    "                lecture_soup = BeautifulSoup(lecture_response.content, 'html.parser')\n",
    "\n",
    "                lecture_main_content = lecture_soup.find('div', id='main-content')\n",
    "\n",
    "                if lecture_main_content:\n",
    "                    lecture_content = lecture_main_content.text.strip()\n",
    "\n",
    "                    filename = os.path.join(output_folder, f\"{lecture_title}.txt\")\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"Lecture: {lecture_title}\\n\\n\")\n",
    "                        f.write(lecture_content)\n",
    "                    \n",
    "                    lecture_notes.append((lecture_title, lecture_content))\n",
    "                else:\n",
    "                    print(f\"Warning: No main content found for lecture: {lecture_url}\")\n",
    "        else:\n",
    "            print(f\"Error: No main content div found on the page: {base_url}\")\n",
    "\n",
    "        return lecture_notes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching lecture notes: {e}\")\n",
    "        return []\n",
    "\n",
    "lecture_notes = scrape_lecture_notes(base_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing file: E:\\Assignment2\\data\\milestone_papers.csv\n",
      "Milestone papers table saved as E:\\Assignment2\\data\\milestone_papers.csv\n",
      "Milestone Papers Table:\n",
      "      Date     Keywords Institute  \\\n",
      "1  2018-06      GPT 1.0    OpenAI   \n",
      "2  2018-10         BERT    Google   \n",
      "3  2019-02      GPT 2.0    OpenAI   \n",
      "4  2019-09  Megatron-LM    NVIDIA   \n",
      "5  2019-10           T5    Google   \n",
      "\n",
      "                                               Paper Publication  \n",
      "1  Improving Language Understanding by Generative...         NaN  \n",
      "2  BERT: Pre-training of Deep Bidirectional Trans...       NAACL  \n",
      "3  Language Models are Unsupervised Multitask Lea...         NaN  \n",
      "4  Megatron-LM: Training Multi-Billion Parameter ...         NaN  \n",
      "5  Exploring the Limits of Transfer Learning with...        JMLR  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "github_url = \"https://github.com/Hannibal046/Awesome-LLM#milestone-papers\"\n",
    "\n",
    "def scrape_milestone_papers(github_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(github_url)\n",
    "        response.raise_for_status()  # Raise an exception for bad response status\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        milestone_table = None\n",
    "        for table in tables:\n",
    "            if not table.has_attr('class') and not table.has_attr('id'):\n",
    "                milestone_table = table\n",
    "                break\n",
    "\n",
    "        if milestone_table:\n",
    "            df = pd.read_html(str(milestone_table))[0]\n",
    "\n",
    "            df.columns = ['Date', 'Keywords', 'Institute', 'Paper', 'Publication']\n",
    "\n",
    "            df = df.iloc[1:]\n",
    "\n",
    "            if os.path.exists(save_path):\n",
    "                os.remove(save_path)\n",
    "                print(f\"Deleted existing file: {save_path}\")\n",
    "\n",
    "            output_folder = os.path.dirname(save_path)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print(f\"Milestone papers table saved as {save_path}\")\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Error: No suitable table found on the page.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching milestone papers: {e}\")\n",
    "        return None\n",
    "\n",
    "save_path = r\"E:\\Assignment2\\data\\milestone_papers.csv\"\n",
    "milestone_papers_df = scrape_milestone_papers(github_url, save_path)\n",
    "if milestone_papers_df is not None:\n",
    "    print(\"Milestone Papers Table:\")\n",
    "    print(milestone_papers_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting PDF text from Improving Language Understanding by Generative Pre-Training: Invalid URL 'Improving Language Understanding by Generative Pre-Training': No scheme supplied. Perhaps you meant https://Improving Language Understanding by Generative Pre-Training?\n",
      "Saved text from 'Improving Language Understanding by Generative Pre-Training' PDF as 'Improving Language Understanding by Generative Pre-Training.txt'\n",
      "Error extracting PDF text from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: No connection adapters were found for 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\n",
      "Saved text from 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' PDF as 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.txt'\n",
      "Error extracting PDF text from Language Models are Unsupervised Multitask Learners: Invalid URL 'Language Models are Unsupervised Multitask Learners': No scheme supplied. Perhaps you meant https://Language Models are Unsupervised Multitask Learners?\n",
      "Saved text from 'Language Models are Unsupervised Multitask Learners' PDF as 'Language Models are Unsupervised Multitask Learners.txt'\n",
      "Error extracting PDF text from Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism: No connection adapters were found for 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism'\n",
      "Saved text from 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism' PDF as 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.txt'\n",
      "Error extracting PDF text from Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer: Invalid URL 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer': No scheme supplied. Perhaps you meant https://Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer?\n",
      "Saved text from 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer' PDF as 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.txt'\n",
      "Error extracting PDF text from ZeRO: Memory Optimizations Toward Training Trillion Parameter Models: No connection adapters were found for 'ZeRO: Memory Optimizations Toward Training Trillion Parameter Models'\n",
      "Saved text from 'ZeRO: Memory Optimizations Toward Training Trillion Parameter Models' PDF as 'ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.txt'\n",
      "Error extracting PDF text from Scaling Laws for Neural Language Models: Invalid URL 'Scaling Laws for Neural Language Models': No scheme supplied. Perhaps you meant https://Scaling Laws for Neural Language Models?\n",
      "Saved text from 'Scaling Laws for Neural Language Models' PDF as 'Scaling Laws for Neural Language Models.txt'\n",
      "Error extracting PDF text from Language models are few-shot learners: Invalid URL 'Language models are few-shot learners': No scheme supplied. Perhaps you meant https://Language models are few-shot learners?\n",
      "Saved text from 'Language models are few-shot learners' PDF as 'Language models are few-shot learners.txt'\n",
      "Error extracting PDF text from Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity: No connection adapters were found for 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity'\n",
      "Saved text from 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity' PDF as 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.txt'\n",
      "Error extracting PDF text from Evaluating Large Language Models Trained on Code: Invalid URL 'Evaluating Large Language Models Trained on Code': No scheme supplied. Perhaps you meant https://Evaluating Large Language Models Trained on Code?\n",
      "Saved text from 'Evaluating Large Language Models Trained on Code' PDF as 'Evaluating Large Language Models Trained on Code.txt'\n",
      "Error extracting PDF text from On the Opportunities and Risks of Foundation Models: Invalid URL 'On the Opportunities and Risks of Foundation Models': No scheme supplied. Perhaps you meant https://On the Opportunities and Risks of Foundation Models?\n",
      "Saved text from 'On the Opportunities and Risks of Foundation Models' PDF as 'On the Opportunities and Risks of Foundation Models.txt'\n",
      "Error extracting PDF text from Finetuned Language Models are Zero-Shot Learners: Invalid URL 'Finetuned Language Models are Zero-Shot Learners': No scheme supplied. Perhaps you meant https://Finetuned Language Models are Zero-Shot Learners?\n",
      "Saved text from 'Finetuned Language Models are Zero-Shot Learners' PDF as 'Finetuned Language Models are Zero-Shot Learners.txt'\n",
      "Error extracting PDF text from Multitask Prompted Training Enables Zero-Shot Task Generalization: Invalid URL 'Multitask Prompted Training Enables Zero-Shot Task Generalization': No scheme supplied. Perhaps you meant https://Multitask Prompted Training Enables Zero-Shot Task Generalization?\n",
      "Saved text from 'Multitask Prompted Training Enables Zero-Shot Task Generalization' PDF as 'Multitask Prompted Training Enables Zero-Shot Task Generalization.txt'\n",
      "Error extracting PDF text from GLaM: Efficient Scaling of Language Models with Mixture-of-Experts: No connection adapters were found for 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts'\n",
      "Saved text from 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts' PDF as 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.txt'\n",
      "Error extracting PDF text from WebGPT: Browser-assisted question-answering with human feedback: No connection adapters were found for 'WebGPT: Browser-assisted question-answering with human feedback'\n",
      "Saved text from 'WebGPT: Browser-assisted question-answering with human feedback' PDF as 'WebGPT: Browser-assisted question-answering with human feedback.txt'\n",
      "Error extracting PDF text from Improving language models by retrieving from trillions of tokens: Invalid URL 'Improving language models by retrieving from trillions of tokens': No scheme supplied. Perhaps you meant https://Improving language models by retrieving from trillions of tokens?\n",
      "Saved text from 'Improving language models by retrieving from trillions of tokens' PDF as 'Improving language models by retrieving from trillions of tokens.txt'\n",
      "Error extracting PDF text from Scaling Language Models: Methods, Analysis & Insights from Training Gopher: No connection adapters were found for 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher'\n",
      "Saved text from 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher' PDF as 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher.txt'\n",
      "Error extracting PDF text from Chain-of-Thought Prompting Elicits Reasoning in Large Language Models: Invalid URL 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models': No scheme supplied. Perhaps you meant https://Chain-of-Thought Prompting Elicits Reasoning in Large Language Models?\n",
      "Saved text from 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models' PDF as 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.txt'\n",
      "Error extracting PDF text from LaMDA: Language Models for Dialog Applications: No connection adapters were found for 'LaMDA: Language Models for Dialog Applications'\n",
      "Saved text from 'LaMDA: Language Models for Dialog Applications' PDF as 'LaMDA: Language Models for Dialog Applications.txt'\n",
      "Error extracting PDF text from Solving Quantitative Reasoning Problems with Language Models: Invalid URL 'Solving Quantitative Reasoning Problems with Language Models': No scheme supplied. Perhaps you meant https://Solving Quantitative Reasoning Problems with Language Models?\n",
      "Saved text from 'Solving Quantitative Reasoning Problems with Language Models' PDF as 'Solving Quantitative Reasoning Problems with Language Models.txt'\n",
      "Error extracting PDF text from Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model: Invalid URL 'Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model': No scheme supplied. Perhaps you meant https://Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model?\n",
      "Saved text from 'Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model' PDF as 'Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.txt'\n",
      "Error extracting PDF text from Training language models to follow instructions with human feedback: Invalid URL 'Training language models to follow instructions with human feedback': No scheme supplied. Perhaps you meant https://Training language models to follow instructions with human feedback?\n",
      "Saved text from 'Training language models to follow instructions with human feedback' PDF as 'Training language models to follow instructions with human feedback.txt'\n",
      "Error extracting PDF text from PaLM: Scaling Language Modeling with Pathways: No connection adapters were found for 'PaLM: Scaling Language Modeling with Pathways'\n",
      "Saved text from 'PaLM: Scaling Language Modeling with Pathways' PDF as 'PaLM: Scaling Language Modeling with Pathways.txt'\n",
      "Error extracting PDF text from An empirical analysis of compute-optimal large language model training: Invalid URL 'An empirical analysis of compute-optimal large language model training': No scheme supplied. Perhaps you meant https://An empirical analysis of compute-optimal large language model training?\n",
      "Saved text from 'An empirical analysis of compute-optimal large language model training' PDF as 'An empirical analysis of compute-optimal large language model training.txt'\n",
      "Error extracting PDF text from OPT: Open Pre-trained Transformer Language Models: No connection adapters were found for 'OPT: Open Pre-trained Transformer Language Models'\n",
      "Saved text from 'OPT: Open Pre-trained Transformer Language Models' PDF as 'OPT: Open Pre-trained Transformer Language Models.txt'\n",
      "Error extracting PDF text from Unifying Language Learning Paradigms: Invalid URL 'Unifying Language Learning Paradigms': No scheme supplied. Perhaps you meant https://Unifying Language Learning Paradigms?\n",
      "Saved text from 'Unifying Language Learning Paradigms' PDF as 'Unifying Language Learning Paradigms.txt'\n",
      "Error extracting PDF text from Emergent Abilities of Large Language Models: Invalid URL 'Emergent Abilities of Large Language Models': No scheme supplied. Perhaps you meant https://Emergent Abilities of Large Language Models?\n",
      "Saved text from 'Emergent Abilities of Large Language Models' PDF as 'Emergent Abilities of Large Language Models.txt'\n",
      "Error extracting PDF text from Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models: No connection adapters were found for 'Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models'\n",
      "Saved text from 'Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models' PDF as 'Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.txt'\n",
      "Error extracting PDF text from Language Models are General-Purpose Interfaces: Invalid URL 'Language Models are General-Purpose Interfaces': No scheme supplied. Perhaps you meant https://Language Models are General-Purpose Interfaces?\n",
      "Saved text from 'Language Models are General-Purpose Interfaces' PDF as 'Language Models are General-Purpose Interfaces.txt'\n",
      "Error extracting PDF text from Improving alignment of dialogue agents via targeted human judgements: Invalid URL 'Improving alignment of dialogue agents via targeted human judgements': No scheme supplied. Perhaps you meant https://Improving alignment of dialogue agents via targeted human judgements?\n",
      "Saved text from 'Improving alignment of dialogue agents via targeted human judgements' PDF as 'Improving alignment of dialogue agents via targeted human judgements.txt'\n",
      "Error extracting PDF text from Scaling Instruction-Finetuned Language Models: Invalid URL 'Scaling Instruction-Finetuned Language Models': No scheme supplied. Perhaps you meant https://Scaling Instruction-Finetuned Language Models?\n",
      "Saved text from 'Scaling Instruction-Finetuned Language Models' PDF as 'Scaling Instruction-Finetuned Language Models.txt'\n",
      "Error extracting PDF text from GLM-130B: An Open Bilingual Pre-trained Model: No connection adapters were found for 'GLM-130B: An Open Bilingual Pre-trained Model'\n",
      "Saved text from 'GLM-130B: An Open Bilingual Pre-trained Model' PDF as 'GLM-130B: An Open Bilingual Pre-trained Model.txt'\n",
      "Error extracting PDF text from Holistic Evaluation of Language Models: Invalid URL 'Holistic Evaluation of Language Models': No scheme supplied. Perhaps you meant https://Holistic Evaluation of Language Models?\n",
      "Saved text from 'Holistic Evaluation of Language Models' PDF as 'Holistic Evaluation of Language Models.txt'\n",
      "Error extracting PDF text from BLOOM: A 176B-Parameter Open-Access Multilingual Language Model: No connection adapters were found for 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model'\n",
      "Saved text from 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model' PDF as 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.txt'\n",
      "Error extracting PDF text from Galactica: A Large Language Model for Science: No connection adapters were found for 'Galactica: A Large Language Model for Science'\n",
      "Saved text from 'Galactica: A Large Language Model for Science' PDF as 'Galactica: A Large Language Model for Science.txt'\n",
      "Error extracting PDF text from OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization: No connection adapters were found for 'OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization'\n",
      "Saved text from 'OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization' PDF as 'OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization.txt'\n",
      "Error extracting PDF text from The Flan Collection: Designing Data and Methods for Effective Instruction Tuning: No connection adapters were found for 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning'\n",
      "Saved text from 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning' PDF as 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning.txt'\n",
      "Error extracting PDF text from LLaMA: Open and Efficient Foundation Language Models: No connection adapters were found for 'LLaMA: Open and Efficient Foundation Language Models'\n",
      "Saved text from 'LLaMA: Open and Efficient Foundation Language Models' PDF as 'LLaMA: Open and Efficient Foundation Language Models.txt'\n",
      "Error extracting PDF text from Language Is Not All You Need: Aligning Perception with Language Models: No connection adapters were found for 'Language Is Not All You Need: Aligning Perception with Language Models'\n",
      "Saved text from 'Language Is Not All You Need: Aligning Perception with Language Models' PDF as 'Language Is Not All You Need: Aligning Perception with Language Models.txt'\n",
      "Error extracting PDF text from PaLM-E: An Embodied Multimodal Language Model: No connection adapters were found for 'PaLM-E: An Embodied Multimodal Language Model'\n",
      "Saved text from 'PaLM-E: An Embodied Multimodal Language Model' PDF as 'PaLM-E: An Embodied Multimodal Language Model.txt'\n",
      "Error extracting PDF text from GPT-4 Technical Report: Invalid URL 'GPT-4 Technical Report': No scheme supplied. Perhaps you meant https://GPT-4 Technical Report?\n",
      "Saved text from 'GPT-4 Technical Report' PDF as 'GPT-4 Technical Report.txt'\n",
      "Error extracting PDF text from Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling: No connection adapters were found for 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling'\n",
      "Saved text from 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling' PDF as 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling.txt'\n",
      "Error extracting PDF text from Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision: Invalid URL 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision': No scheme supplied. Perhaps you meant https://Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision?\n",
      "Saved text from 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision' PDF as 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision.txt'\n",
      "Error extracting PDF text from PaLM 2 Technical Report: Invalid URL 'PaLM 2 Technical Report': No scheme supplied. Perhaps you meant https://PaLM 2 Technical Report?\n",
      "Saved text from 'PaLM 2 Technical Report' PDF as 'PaLM 2 Technical Report.txt'\n",
      "Error extracting PDF text from RWKV: Reinventing RNNs for the Transformer Era: No connection adapters were found for 'RWKV: Reinventing RNNs for the Transformer Era'\n",
      "Saved text from 'RWKV: Reinventing RNNs for the Transformer Era' PDF as 'RWKV: Reinventing RNNs for the Transformer Era.txt'\n",
      "Error extracting PDF text from Direct Preference Optimization: Your Language Model is Secretly a Reward Model: No connection adapters were found for 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model'\n",
      "Saved text from 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model' PDF as 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model.txt'\n",
      "Error extracting PDF text from Tree of Thoughts: Deliberate Problem Solving with Large Language Models: No connection adapters were found for 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'\n",
      "Saved text from 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models' PDF as 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models.txt'\n",
      "Error extracting PDF text from Llama 2: Open Foundation and Fine-Tuned Chat Models: No connection adapters were found for 'Llama 2: Open Foundation and Fine-Tuned Chat Models'\n",
      "Saved text from 'Llama 2: Open Foundation and Fine-Tuned Chat Models' PDF as 'Llama 2: Open Foundation and Fine-Tuned Chat Models.txt'\n",
      "Error extracting PDF text from Mistral 7B: Invalid URL 'Mistral 7B': No scheme supplied. Perhaps you meant https://Mistral 7B?\n",
      "Saved text from 'Mistral 7B' PDF as 'Mistral 7B.txt'\n",
      "Error extracting PDF text from Mamba: Linear-Time Sequence Modeling with Selective State Spaces: No connection adapters were found for 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces'\n",
      "Saved text from 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces' PDF as 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces.txt'\n",
      "Error extracting PDF text from Jamba: A Hybrid Transformer-Mamba Language Model: No connection adapters were found for 'Jamba: A Hybrid Transformer-Mamba Language Model'\n",
      "Saved text from 'Jamba: A Hybrid Transformer-Mamba Language Model' PDF as 'Jamba: A Hybrid Transformer-Mamba Language Model.txt'\n",
      "Milestone Papers Table:\n",
      "      Date     Keywords Institute  \\\n",
      "1  2018-06      GPT 1.0    OpenAI   \n",
      "2  2018-10         BERT    Google   \n",
      "3  2019-02      GPT 2.0    OpenAI   \n",
      "4  2019-09  Megatron-LM    NVIDIA   \n",
      "5  2019-10           T5    Google   \n",
      "\n",
      "                                               Paper Publication  \n",
      "1  Improving Language Understanding by Generative...         NaN  \n",
      "2  BERT: Pre-training of Deep Bidirectional Trans...       NAACL  \n",
      "3  Language Models are Unsupervised Multitask Lea...         NaN  \n",
      "4  Megatron-LM: Training Multi-Billion Parameter ...         NaN  \n",
      "5  Exploring the Limits of Transfer Learning with...        JMLR  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "github_url = \"https://github.com/Hannibal046/Awesome-LLM#milestone-papers\"\n",
    "\n",
    "def scrape_milestone_papers(github_url):\n",
    "    try:\n",
    "        response = requests.get(github_url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        milestone_table = None\n",
    "        for table in tables:\n",
    "            if not table.has_attr('class') and not table.has_attr('id'):\n",
    "                milestone_table = table\n",
    "                break\n",
    "\n",
    "        if milestone_table:\n",
    "            df = pd.read_html(str(milestone_table))[0]\n",
    "\n",
    "            paper_links = extract_links_from_paper_column(df, 'Paper')\n",
    "\n",
    "            return paper_links\n",
    "        else:\n",
    "            print(\"Error: No suitable table found on the page.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching milestone papers: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_links_from_paper_column(df, column_name):\n",
    "    links = []\n",
    "    for index, row in df.iterrows():\n",
    "        paper_data = row[column_name]\n",
    "        soup = BeautifulSoup(paper_data, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            links.append(link['href'])\n",
    "    return links\n",
    "\n",
    "paper_links = scrape_milestone_papers(github_url)\n",
    "if paper_links:\n",
    "    print(\"Extracted Paper Links:\")\n",
    "    for link in paper_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from PyPDF2) (4.12.2)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing file: E:\\Assignment2\\data\\milestone_papers_text\\BERT.txt\n",
      "Deleted existing file: E:\\Assignment2\\data\\milestone_papers_text\\GPT 1.0.txt\n",
      "All links found in the milestone papers table:\n",
      "['https://arxiv.org/pdf/1706.03762.pdf', 'https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf', 'https://aclanthology.org/N19-1423.pdf', 'https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf', 'https://arxiv.org/pdf/1909.08053.pdf', 'https://jmlr.org/papers/v21/20-074.html', 'https://arxiv.org/pdf/1910.02054.pdf', 'https://arxiv.org/pdf/2001.08361.pdf', 'https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf', 'https://arxiv.org/pdf/2101.03961.pdf', 'https://arxiv.org/pdf/2107.03374.pdf', 'https://arxiv.org/pdf/2108.07258.pdf', 'https://openreview.net/forum?id=gEZrGCozdqR', 'https://arxiv.org/abs/2110.08207', 'https://arxiv.org/pdf/2112.06905.pdf', 'https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22', 'https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens', 'https://arxiv.org/pdf/2112.11446.pdf', 'https://arxiv.org/pdf/2201.11903.pdf', 'https://arxiv.org/pdf/2201.08239.pdf', 'https://arxiv.org/abs/2206.14858', 'https://arxiv.org/pdf/2201.11990.pdf', 'https://arxiv.org/pdf/2203.02155.pdf', 'https://arxiv.org/pdf/2204.02311.pdf', 'https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training', 'https://arxiv.org/pdf/2205.01068.pdf', 'https://arxiv.org/abs/2205.05131v1', 'https://openreview.net/pdf?id=yzkSU5zdwD', 'https://github.com/google/BIG-bench', 'https://arxiv.org/pdf/2206.06336.pdf', 'https://arxiv.org/pdf/2209.14375.pdf', 'https://arxiv.org/pdf/2210.11416.pdf', 'https://arxiv.org/pdf/2210.02414.pdf', 'https://arxiv.org/pdf/2211.09110.pdf', 'https://arxiv.org/pdf/2211.05100.pdf', 'https://arxiv.org/pdf/2211.09085.pdf', 'https://arxiv.org/pdf/2212.12017', 'https://arxiv.org/pdf/2301.13688.pdf', 'https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/', 'https://arxiv.org/abs/2302.14045', 'https://palm-e.github.io', 'https://openai.com/research/gpt-4', 'https://arxiv.org/abs/2304.01373', 'https://arxiv.org/abs/2305.03047', 'https://ai.google/static/documents/palm2techreport.pdf', 'https://arxiv.org/abs/2305.13048', 'https://arxiv.org/pdf/2305.18290.pdf', 'https://arxiv.org/pdf/2305.10601.pdf', 'https://arxiv.org/pdf/2307.09288.pdf', 'https://arxiv.org/pdf/2310.06825.pdf', 'https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf', 'https://arxiv.org/pdf/2403.19887']\n",
      "Saved text from PDF 1 as 'GPT 1.0.txt'\n",
      "Saved text from PDF 2 as 'BERT.txt'\n",
      "Saved text from PDF 3 as 'GPT 2.0.txt'\n",
      "Saved text from PDF 4 as 'Megatron-LM.txt'\n",
      "Saved text from PDF 5 as 'T5.txt'\n",
      "Error extracting PDF text from https://jmlr.org/papers/v21/20-074.html: EOF marker not found\n",
      "Saved text from PDF 6 as 'ZeRO.txt'\n",
      "Saved text from PDF 7 as 'Scaling Law.txt'\n",
      "Saved text from PDF 8 as 'GPT 3.0.txt'\n",
      "Saved text from PDF 9 as 'Switch Transformers.txt'\n",
      "Saved text from PDF 10 as 'Codex.txt'\n",
      "Saved text from PDF 11 as 'Foundation Models.txt'\n",
      "Saved text from PDF 12 as 'FLAN.txt'\n",
      "Error extracting PDF text from https://openreview.net/forum?id=gEZrGCozdqR: EOF marker not found\n",
      "Saved text from PDF 13 as 'T0.txt'\n",
      "Error extracting PDF text from https://arxiv.org/abs/2110.08207: EOF marker not found\n",
      "Saved text from PDF 14 as 'GLaM.txt'\n",
      "Saved text from PDF 15 as 'WebGPT.txt'\n",
      "Error extracting PDF text from https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22: Cannot read an empty file\n",
      "Saved text from PDF 16 as 'Retro.txt'\n",
      "Error extracting PDF text from https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens: EOF marker not found\n",
      "Saved text from PDF 17 as 'Gopher.txt'\n",
      "Saved text from PDF 18 as 'COT.txt'\n",
      "Saved text from PDF 19 as 'LaMDA.txt'\n",
      "Saved text from PDF 20 as 'Minerva.txt'\n",
      "Error extracting PDF text from https://arxiv.org/abs/2206.14858: EOF marker not found\n",
      "Saved text from PDF 21 as 'Megatron-Turing NLG.txt'\n",
      "Saved text from PDF 22 as 'InstructGPT.txt'\n",
      "Saved text from PDF 23 as 'PaLM.txt'\n",
      "Saved text from PDF 24 as 'Chinchilla.txt'\n",
      "Error extracting PDF text from https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training: EOF marker not found\n",
      "Saved text from PDF 25 as 'OPT.txt'\n",
      "Saved text from PDF 26 as 'UL2.txt'\n",
      "Error extracting PDF text from https://arxiv.org/abs/2205.05131v1: EOF marker not found\n",
      "Saved text from PDF 27 as 'Emergent Abilities.txt'\n",
      "Saved text from PDF 28 as 'BIG-bench.txt'\n",
      "Error extracting PDF text from https://github.com/google/BIG-bench: EOF marker not found\n",
      "Saved text from PDF 29 as 'METALM.txt'\n",
      "Saved text from PDF 30 as 'Sparrow.txt'\n",
      "Error fetching milestone papers: [Errno 2] No such file or directory: 'E:\\\\Assignment2\\\\data\\\\milestone_papers_text\\\\Flan-T5/PaLM.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "github_url = \"https://github.com/Hannibal046/Awesome-LLM#milestone-papers\"\n",
    "\n",
    "def scrape_milestone_papers(github_url, save_folder):\n",
    "    try:\n",
    "        response = requests.get(github_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        milestone_table = None\n",
    "        for table in tables:\n",
    "            if not table.has_attr('class') and not table.has_attr('id'):\n",
    "                milestone_table = table\n",
    "                break\n",
    "\n",
    "        if milestone_table is None:\n",
    "            raise Exception(\"Error: No suitable table found in milestone papers section.\")\n",
    "\n",
    "        df = pd.read_html(str(milestone_table))[0]\n",
    "        df.columns = ['Date', 'Keywords', 'Institute', 'Paper', 'Publication']\n",
    "        df = df.iloc[1:]\n",
    "\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        for filename in os.listdir(save_folder):\n",
    "            file_path = os.path.join(save_folder, filename)\n",
    "            try:\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted existing file: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file_path}: {e}\")\n",
    "\n",
    "        all_links = milestone_table.find_all('a', href=True)\n",
    "\n",
    "        valid_href_values = []\n",
    "        for link in all_links:\n",
    "            if 'target' in link.attrs and link['target'] == '_blank':\n",
    "                continue\n",
    "            valid_href_values.append(link['href'])\n",
    "\n",
    "        print(f\"All links found in the milestone papers table:\")\n",
    "        print(valid_href_values)\n",
    "\n",
    "        for idx, link in enumerate(valid_href_values):\n",
    "            pdf_text = extract_pdf_text(link)\n",
    "\n",
    "            keyword = df.iloc[idx]['Keywords']\n",
    "            if not keyword:\n",
    "                keyword = f\"pdf{idx + 1}\"\n",
    "\n",
    "            txt_filename = f\"{keyword}.txt\"\n",
    "            txt_filepath = os.path.join(save_folder, txt_filename)\n",
    "            with open(txt_filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(pdf_text)\n",
    "            print(f\"Saved text from PDF {idx + 1} as '{txt_filename}'\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request exception occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching milestone papers: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_pdf_text(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        temp_pdf_path = 'temp.pdf'\n",
    "        with open(temp_pdf_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        pdf_text = ''\n",
    "        with open(temp_pdf_path, 'rb') as f:\n",
    "            pdf_reader = PdfReader(f)\n",
    "            for page in pdf_reader.pages:\n",
    "                pdf_text += page.extract_text()\n",
    "\n",
    "        os.remove(temp_pdf_path)\n",
    "\n",
    "        return pdf_text.strip()\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading PDF from {pdf_url}: {e}\")\n",
    "        return ''\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request exception occurred while downloading PDF from {pdf_url}: {e}\")\n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF text from {pdf_url}: {e}\")\n",
    "        return ''\n",
    "\n",
    "# Example usage\n",
    "save_folder = r\"E:\\Assignment2\\data\\milestone_papers_text\"\n",
    "milestone_papers_df = scrape_milestone_papers(github_url, save_folder)\n",
    "if milestone_papers_df is not None:\n",
    "    print(\"Milestone Papers Table:\")\n",
    "    print(milestone_papers_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (0.2.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (0.2.9)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (0.1.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-community) (8.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (2.7.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in d:\\anaconda\\envs\\env2\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (2.18.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Ensure the environment variable is set\n",
    "api_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "if not api_key:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN environment variable not set\")\n",
    "\n",
    "# Initialize LLM with API key\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-small', \n",
    "    model_kwargs={'temperature': 0.7, 'max_length': 512}\n",
    ")\n",
    "\n",
    "# Define prompts for each step in the sequential chain\n",
    "preprocess_prompt = PromptTemplate.from_template(\"Preprocess the document chunk to extract key information: {chunk}\")\n",
    "analysis_prompt = PromptTemplate.from_template(\"Analyze the key information from the preprocessed chunk: {key_info}\")\n",
    "\n",
    "# Create the SequentialChain\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[\n",
    "        LLMChain(llm=llm, prompt=preprocess_prompt, output_key=\"key_info\"),\n",
    "        LLMChain(llm=llm, prompt=analysis_prompt, output_key=\"analysis\")\n",
    "    ],\n",
    "    \n",
    "    input_variables=[\"chunk\"],\n",
    "    output_variables=[\"analysis\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents(directory_path):\n",
    "    loader = DirectoryLoader(directory_path)\n",
    "    documents = loader.load()\n",
    "    return [doc.page_content for doc in documents]  # Access the content attribute directly\n",
    "\n",
    "def preprocess_documents(documents, chunk_size=512, overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(splitter.split_text(doc))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def analyze_query(doc_chunks, query):\n",
    "    results = []\n",
    "    for chunk in doc_chunks:\n",
    "        analysis = sequential_chain({\"chunk\": chunk})\n",
    "        results.append(analysis['analysis'])\n",
    "    # Aggregate the results\n",
    "    integrated_response = aggregate_results(results, query)\n",
    "    return integrated_response\n",
    "\n",
    "def aggregate_results(results, query):\n",
    "    # Integrate the results to form a coherent response to the query\n",
    "    integrated_response = \" \".join(results)\n",
    "    return integrated_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/google/flan-t5-small (Request ID: 1AgQcmt0E3Pifuo02sZlM)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/google/flan-t5-small",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are some milestone model architectures and papers in the last few years?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Analyze query using the document chunks\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m, in \u001b[0;36manalyze_query\u001b[1;34m(doc_chunks, query)\u001b[0m\n\u001b[0;32m     15\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc_chunks:\n\u001b[1;32m---> 17\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[43msequential_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(analysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Aggregate the results\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\sequential.py:105\u001b[0m, in \u001b[0;36mSequentialChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chain \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchains):\n\u001b[0;32m    104\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[1;32m--> 105\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     known_values\u001b[38;5;241m.\u001b[39mupdate(outputs)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {k: known_values[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_variables}\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    381\u001b[0m }\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1317\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1314\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1316\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1317\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1320\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:139\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    137\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 139\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\env2\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/google/flan-t5-small (Request ID: 1AgQcmt0E3Pifuo02sZlM)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Load and preprocess documents\n",
    "    documents = load_documents('E:/Assignment2/data/milestone_papers_text')\n",
    "    doc_chunks = preprocess_documents(documents)\n",
    "\n",
    "    # Query to be answered\n",
    "    query = \"What are some milestone model architectures and papers in the last few years?\"\n",
    "\n",
    "    # Analyze query using the document chunks\n",
    "    response = analyze_query(doc_chunks, query)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize LLM\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-small', model_kwargs={'temperature': 0.7, 'max_length': 512})\n",
    "\n",
    "# Define prompts for each step in the sequential chain\n",
    "preprocess_prompt = PromptTemplate(\"Preprocess the document chunk to extract key information: {chunk}\")\n",
    "analysis_prompt = PromptTemplate(\"Analyze the key information from the preprocessed chunk: {key_info}\")\n",
    "\n",
    "# Create the SequentialChain\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[\n",
    "        LLMChain(llm=llm, prompt=preprocess_prompt, output_key=\"key_info\"),\n",
    "        LLMChain(llm=llm, prompt=analysis_prompt, output_key=\"analysis\")\n",
    "    ],\n",
    "    input_key=\"chunk\",\n",
    "    output_key=\"final_analysis\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(doc_chunks, query):\n",
    "    results = []\n",
    "    for chunk in doc_chunks:\n",
    "        analysis = sequential_chain({\"chunk\": chunk})\n",
    "        results.append(analysis['final_analysis'])\n",
    "    # Aggregate the results\n",
    "    integrated_response = aggregate_results(results, query)\n",
    "    return integrated_response\n",
    "\n",
    "def aggregate_results(results, query):\n",
    "    # Integrate the results to form a coherent response to the query\n",
    "    integrated_response = \" \".join(results)\n",
    "    return integrated_response\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Load and preprocess documents\n",
    "    documents = load_documents('E:\\Assignment2\\data\\milestone_papers_text')\n",
    "    doc_chunks = preprocess_documents(documents)\n",
    "\n",
    "    # Query to be answered\n",
    "    query = \"What are some milestone model architectures and papers in the last few years?\"\n",
    "\n",
    "    # Analyze query using the document chunks\n",
    "    response = analyze_query(doc_chunks, query)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_documents(directory_path):\n",
    "    loader = DirectoryLoader(directory_path)\n",
    "    documents = loader.load()\n",
    "    return [doc.page_content for doc in documents]\n",
    "\n",
    "def preprocess_documents(documents, chunk_size=512, overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(splitter.split_text(doc))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar chunks:\n",
      "CSCW (2019).\n",
      "\n",
      "Nayeon Lee, Yejin Bang, Andrea Madotto, Madian Khabsa, and Pascale Fung. 2021a. Towards Few-shot Fact-Checking via\n",
      "\n",
      "Perplexity. In NAACL .\n",
      "\n",
      "Yong Jae Lee, Joydeep Ghosh, and K. Grauman. 2012. Discovering important people and objects for egocentric video\n",
      "\n",
      "summarization. 2012 IEEE Conference on Computer Vision and Pattern Recognition (2012), 13461353.\n",
      "\n",
      "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2021. FNet: Mixing Tokens with Fourier Transforms.\n",
      "exhibit harmful biases targeting specific groups (e.g., gender and racial bias) [Bender et al .2021;\n",
      "\n",
      "Basta et al .2019; Kurita et al .2019; Sheng et al .2019] and can produce toxic outputs [Gehman et al .\n",
      "\n",
      "2020] (5.2: misuse ). While strategies such as further fine-tuning the foundation model on carefully\n",
      "\n",
      "curated datasets (for potentially multiple generations) [Solaiman and Dennison 2021] or applying\n",
      "\n",
      "controllable generation techniques [Keskar et al .2019] have shown some success in mitigating\n",
      "Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Javier Ortiz Surez, Iroro Orife, Kelechi Ogueji,\n",
      "\n",
      "Rubungo Andre Niyongabo, Toan Q. Nguyen, Mathias Mller, Andr Mller, Shamsuddeen Hassan Muhammad, Nanda\n",
      "\n",
      "Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha\n",
      "\n",
      "Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\n",
      "these metaphysical characterizations we adopt.\n",
      "\n",
      "Internalism and referentialism can both be cast as defining a mapping problem: to associate a\n",
      "\n",
      "linguistic sign with a meaning or a semantic value. For internalism this will be a representation or\n",
      "\n",
      "concept, a program for computing a value, or some other type of internal object. For referentialism,\n",
      "\n",
      "it might be a mapping from a word to an external referent, or a mapping from a situation to a truth\n",
      "(YES/NO) and which sentence(s) require citing.\n",
      "\n",
      "{text of paper}\n",
      "\n",
      "extract Extract all course titles from the table below:\n",
      "\n",
      "| Title | Lecturer | Room |\n",
      "\n",
      "| Calculus 101 | Smith | Hall B |\n",
      "\n",
      "| Art History | Paz | Hall A |\n",
      "\n",
      "extract Extract all place names from the article below:\n",
      "\n",
      "{news article}\n",
      "\n",
      "extract Given the following list of movie titles, write down any names of cities in the\n",
      "\n",
      "titles.\n",
      "\n",
      "{movie titles}\n",
      "\n",
      "generation Write a creative ad for the following product to run on Facebook aimed at parents:\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = 768  # Dimension of the vector embeddings (adjust as per your model)\n",
    "index = faiss.IndexFlatL2(dimension)  # Example of using L2 distance metric\n",
    "\n",
    "def build_index(doc_chunks, embeddings):\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        index.add(np.array([embedding]))  # Add each document's embedding to the index\n",
    "\n",
    "def search_similar(query_embedding, k=5):\n",
    "    D, I = index.search(np.array([query_embedding]), k)  # Search for k nearest neighbors\n",
    "    return I[0]  # Return indices of the nearest neighbors in the index\n",
    "\n",
    "# Example function to get embeddings (assuming they are precomputed)\n",
    "def get_embeddings(doc_chunks):\n",
    "    embeddings = []\n",
    "    for chunk in doc_chunks:\n",
    "        embedding = compute_embedding(chunk)  # Replace with your embedding function\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Replace with your actual embedding computation function\n",
    "def compute_embedding(text):\n",
    "    # Placeholder function for computing embeddings\n",
    "    return np.random.rand(dimension)  # Example: Random vector\n",
    "\n",
    "# Example function to handle a query\n",
    "def handle_query(query, doc_chunks, embeddings):\n",
    "    query_embedding = compute_embedding(query)  # Compute embedding for the query\n",
    "    similar_indices = search_similar(query_embedding)\n",
    "    similar_chunks = [doc_chunks[i] for i in similar_indices]\n",
    "    return similar_chunks\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess documents\n",
    "    documents = load_documents('E:/Assignment2/data/milestone_papers_text')\n",
    "    doc_chunks = preprocess_documents(documents)\n",
    "\n",
    "    # Compute embeddings for document chunks\n",
    "    embeddings = get_embeddings(doc_chunks)\n",
    "\n",
    "    # Build index with embeddings\n",
    "    build_index(doc_chunks, embeddings)\n",
    "\n",
    "    # Example query\n",
    "    query = \"What are some milestone model architectures and papers in the last few years?\"\n",
    "\n",
    "    # Handle query\n",
    "    similar_chunks = handle_query(query, doc_chunks, embeddings)\n",
    "    print(\"Similar chunks:\")\n",
    "    for chunk in similar_chunks:\n",
    "        print(chunk)\n",
    "    output_folder = r\"E:\\Assignment2\\notebooks\"\n",
    "\n",
    "    # Ensure the folder exists; create it if it doesn't\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Example list of chunks\n",
    "\n",
    "    # Define the filename within the output folder\n",
    "    filename = os.path.join(output_folder, \"a.txt\")\n",
    "\n",
    "    # Writing similar_chunks to a.txt\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for chunk in similar_chunks:\n",
    "            f.write(chunk + '\\n')  \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3968283757.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    output_folder=\"E:\\Assignment2\\notebooks\\\"\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "output_folder=\"E:\\Assignment2\\notebooks\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similar_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Writing similar_chunks to a.txt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43msimilar_chunks\u001b[49m:\n\u001b[0;32m     17\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(chunk \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Write each chunk followed by a newline\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilar chunks written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'similar_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the output folder path\n",
    "output_folder = r\"E:\\Assignment2\\notebooks\"\n",
    "\n",
    "# Ensure the folder exists; create it if it doesn't\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Example list of chunks\n",
    "\n",
    "# Define the filename within the output folder\n",
    "filename = os.path.join(output_folder, \"a.txt\")\n",
    "\n",
    "# Writing similar_chunks to a.txt\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    for chunk in similar_chunks:\n",
    "        f.write(chunk + '\\n')  # Write each chunk followed by a newline\n",
    "\n",
    "print(f\"Similar chunks written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in similar_chunks:\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "output_folder = r\"E:\\Assignment2\\notebooks\"\n",
    "filename = os.path.join(output_folder, \"a.txt\")\n",
    "\n",
    "# Read the contents of the text document\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text_content = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of service, find relevant patents, and conduct other pre-litigation processes in order to ensure\\n\\nthat their clients are at an advantage [Betts and Jaep 2017; Elwany et al .2019; Lippi et al .2019;\\n\\nLee and Hsiang 2019; Hendrycks et al .2021c; Hegel et al .2021]. Notably, recent work has both\\n\\ndescribed the challenges and benefits of using foundation models for contract review [Leivaditi\\n\\net al.2020; Hegel et al .2021; Hendrycks et al .2021c]. In addition to reviewing and drafting legal\\nHerv\\x13 e J\\x13 egou. Large memory layers with product keys. In Advances in Neural Information\\n\\nProcessing Systems , pages 8548{8559, 2019.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\\n\\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\\n\\nbetter. arXiv preprint arXiv:2107.06499 , 2021.\\n\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\\nA:Thecoin wasipped byMaybelle. Sothecoin wasipped 1time, which isanoddnumber.Thecoin started\\n\\nheads up,soafteranoddnumberofips, itwillbetails up. So the answer is no.\\n\\nQ:A coin is heads up. Millicent does not ip the coin. Conception ips the coin. Is the coin still heads up?\\n\\nA:Thecoin wasipped byConception. Sothecoin wasipped 1time, which isanoddnumber.Thecoin\\n\\nstarted heads up,soafteranoddnumberofips, itwillbetails up. So the answer is no.\\nlarge improvements in performance on standardized evaluations will translate into the desired\\n\\noutcomes in the real world (especially if systems are deployed without careful consideration or\\n\\nongoing evaluation). For example, research has shown that judges may re-impose racial prejudice\\n\\nin interpreting the outputs of a risk assessment system [Albright 2019], or otherwise impose their\\n\\nown biases [Stevenson and Doleac 2021]. Ongoing evaluation with proper ecological validity [de\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\n\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\n\\nper-word perplexities.\\n\\nN d model dff h d k dvPdrop lstrain PPL BLEU params\\n\\nsteps (dev) (dev) 106\\n\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n\\n(A)1 512 512 5.29 24.9\\n\\n4 128 128 5.00 25.5\\n\\n16 32 32 4.91 25.8\\n\\n32 16 16 5.01 25.4\\n\\n(B)16 5.16 25.1 58\\n\\n32 5.01 25.4 60\\n\\n(C)2 6.11 23.7 36\\n\\n4 5.19 25.3 50\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'text': 'of service, find relevant patents, and conduct other pre-litigation processes in order to ensure\\n\\nthat their clients are at an advantage [Betts and Jaep 2017; Elwany et al .2019; Lippi et al .2019;\\n\\nLee and Hsiang 2019; Hendrycks et al .2021c; Hegel et al .2021]. Notably, recent work has both\\n\\ndescribed the challenges and benefits of using foundation models for contract review [Leivaditi\\n\\net al.2020; Hegel et al .2021; Hendrycks et al .2021c]. In addition to reviewing and drafting legal\\nHerv\\x13 e J\\x13 egou. Large memory layers with product keys. In Advances in Neural Information\\n\\nProcessing Systems , pages 8548{8559, 2019.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\\n\\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\\n\\nbetter. arXiv preprint arXiv:2107.06499 , 2021.\\n\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\\nA:Thecoin wasipped byMaybelle. Sothecoin wasipped 1time, which isanoddnumber.Thecoin started\\n\\nheads up,soafteranoddnumberofips, itwillbetails up. So the answer is no.\\n\\nQ:A coin is heads up. Millicent does not ip the coin. Conception ips the coin. Is the coin still heads up?\\n\\nA:Thecoin wasipped byConception. Sothecoin wasipped 1time, which isanoddnumber.Thecoin\\n\\nstarted heads up,soafteranoddnumberofips, itwillbetails up. So the answer is no.\\nlarge improvements in performance on standardized evaluations will translate into the desired\\n\\noutcomes in the real world (especially if systems are deployed without careful consideration or\\n\\nongoing evaluation). For example, research has shown that judges may re-impose racial prejudice\\n\\nin interpreting the outputs of a risk assessment system [Albright 2019], or otherwise impose their\\n\\nown biases [Stevenson and Doleac 2021]. Ongoing evaluation with proper ecological validity [de\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\n\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\n\\nper-word perplexities.\\n\\nN d model dff h d k dvPdrop lstrain PPL BLEU params\\n\\nsteps (dev) (dev) 106\\n\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n\\n(A)1 512 512 5.29 24.9\\n\\n4 128 128 5.00 25.5\\n\\n16 32 32 4.91 25.8\\n\\n32 16 16 5.01 25.4\\n\\n(B)16 5.16 25.1 58\\n\\n32 5.01 25.4 60\\n\\n(C)2 6.11 23.7 36\\n\\n4 5.19 25.3 50\\n', 'query': 'What are some milestone model architectures and papers in the last few years?', 'key_info': 'What are some milestone model architectures and papers in the last few years?. Write answer of the above question from the below text. of service, find relevant patents, and conduct other pre-litigation processes in order to ensure\\n\\nthat their clients are at an advantage [Betts and Jaep 2017; Elwany et al .2019; Lippi et al .2019;\\n\\nLee and Hsiang 2019; Hendrycks et al .2021c; Hegel et al .2021]. Notably, recent work has both\\n\\ndescribed the challenges and benefits of using foundation models for contract review [Leivaditi\\n\\net al.2020; Hegel et al .2021; Hendrycks et al .2021c]. In addition to reviewing and drafting legal\\nHerv\\x13 e J\\x13 egou. Large memory layers with product keys. In Advances in Neural Information\\n\\nProcessing Systems , pages 8548{8559, 2019.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\\n\\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\\n\\nbetter. arXiv preprint arXiv:2107.06499 , 2021.\\n\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\\nA:Thecoin wasipped byMaybelle. Sothecoin wasipped 1time, which isanoddnumber.Thecoin started\\n\\nheads up,soafteranoddnumberofips, itwillbetails up. So the answer is no.\\n\\nQ:A coin is heads up. Millicent does not ip the coin. Conception ips the coin. Is the coin still heads up?\\n\\nA:Thecoin wasipped byConception. Sothecoin wasipped 1time, which isanoddnumber.Thecoin\\n\\nstarted heads up,soafteranoddnumberofips, itwillbetails up. So the answer is no.\\nlarge improvements in performance on standardized evaluations will translate into the desired\\n\\noutcomes in the real world (especially if systems are deployed without careful consideration or\\n\\nongoing evaluation). For example, research has shown that judges may re-impose racial prejudice\\n\\nin interpreting the outputs of a risk assessment system [Albright 2019], or otherwise impose their\\n\\nown biases [Stevenson and Doleac 2021]. Ongoing evaluation with proper ecological validity [de\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\n\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\n\\nper-word perplexities.\\n\\nN d model dff h d k dvPdrop lstrain PPL BLEU params\\n\\nsteps (dev) (dev) 106\\n\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n\\n(A)1 512 512 5.29 24.9\\n\\n4 128 128 5.00 25.5\\n\\n16 32 32 4.91 25.8\\n\\n32 16 16 5.01 25.4\\n\\n(B)16 5.16 25.1 58\\n\\n32 5.01 25.4 60\\n\\n(C)2 6.11 23.7 36\\n\\n4 5.19 25.3 50\\n. 2019; Kroll et al.2020]. Moreover, the lack of transparency and accountability in AI\\n\\nsystems can lead to a lack of trust in the decision-making process [Kroll et al.2020; Wang et al.\\n\\n2020]. Therefore, it is crucial to develop AI systems that are transparent, explainable, and\\n\\naccountable [Kroll et al.2020; Wang et al.2020]. Recent work has proposed various methods for\\n\\nimproving the transparency'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Define the path to the text documen\n",
    "# Create HuggingFaceHub configuration\n",
    "llm1 = HuggingFaceHub(\n",
    "    repo_id='meta-llama/Meta-Llama-3-8B-Instruct', \n",
    "    model_kwargs={'temperature': 0.3, 'max_length': 256}\n",
    ")\n",
    "\n",
    "# Define prompts for each step in the sequential chain\n",
    "preprocess_prompt = PromptTemplate.from_template(\"{query}. Write answer of the above question from the below text. {text}. \")\n",
    "analysis_prompt = PromptTemplate.from_template(\"Analysis prompt: {text}. Query: {query}\")\n",
    "\n",
    "# Create the SequentialChain\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[\n",
    "        LLMChain(llm=llm1, prompt=preprocess_prompt, output_key=\"key_info\"),\n",
    "        LLMChain(llm=llm1, prompt=analysis_prompt, output_key=\"analysis\")\n",
    "    ],\n",
    "    input_variables=[\"text\",\"query\"],\n",
    "    output_variables=[\"key_info\"]\n",
    ")\n",
    "\n",
    "# Prompt user for a query\n",
    "query = \"What are some milestone model architectures and papers in the last few years?\"\n",
    "\n",
    "\n",
    "# Process text_content through the SequentialChain\n",
    "response = sequential_chain({\"text\": text_content, \"query\": query})\n",
    "\n",
    "# Print the generated answer\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text=response[\"key_info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content_position = response_text.find(text_content)\n",
    "if text_content_position != -1:\n",
    "    response_text = response_text[text_content_position + len(text_content):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 2019; Kroll et al.2020]. Moreover, the lack of transparency and accountability in AI\n",
      "\n",
      "systems can lead to a lack of trust in the decision-making process [Kroll et al.2020; Wang et al.\n",
      "\n",
      "2020]. Therefore, it is crucial to develop AI systems that are transparent, explainable, and\n",
      "\n",
      "accountable [Kroll et al.2020; Wang et al.2020]. Recent work has proposed various methods for\n",
      "\n",
      "improving the transparency\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"many other researchers and engineers. We acknowledge the immense intellectual and practical contribution the full potential of foundation models, as is envisioned and discussed in detail throughout this report, will rely on research of new architectural and modeling advances to fulfill these desiderata.On the Opportunities and Risks of Foundation Models 81 4.2 Training Authors: Alex Tamkin Training objectives are mathematical functions describing how to transform a model architecture and large amount of broad data into a foundation model. For example, G\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
