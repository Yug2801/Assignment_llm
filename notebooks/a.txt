Was any preprocessing, cleaning, or

labeling of the data done (e.g., dis-

cretization or bucketing, tokenization,

part-of-speech tagging, SIFT feature

extraction, removal of instances, pro-

cessing of missing values)?We remove boilerplate from web pages using proprietary software. We also

remove HTML markup. We extract conversations using a special-purpose

algorithm.

Is the software used to preprocess,

clean, or label the instances available?No.

Uses

Has the dataset been used for any
tokens. So, we were able to split each dataset into a \contaminated" and \clean" subset based on whether at

least 70% of the 8-grams in question, prompt, or target were seen at least once our training data. We report

results on the clean portion vs. the full set in Table 18.

We can see that an equal number of sets have a positive vs. negative accuracy delta on the clean subset,

which would imply that data contamination does not cause meaningful in

ation of our reported results. Note
dataset?CompositionWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Is any information missing from individual instances?Does the dataset contain data that might be considered confidential?Collection processHow was the data associated with each instance acquired?Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?Were any ethical review
way it was collected and prepro-

cessed/cleaned/labeled that might im-

pact future uses?The dataset is static in nature and thus will become progressively more

\stale". It will for example not re

ect new language and norms that evolve

over time. However, due to the nature of the dataset it is relatively cheap

to collect an up-to-date version of the same dataset.

Are there tasks for which the dataset

should not be used?This model should not be used for any of the unacceptable language
tems are better characterized as narrow experts rather than

, **Equal contribution1OpenAI, San Francisco, Califor-

nia, United States. Correspondence to: Alec Radford

<alec@openai.com >.competent generalists. We would like to move towards more

general systems which can perform many tasks â€“ eventually

without the need to manually create and label a training

dataset for each one.

The dominant approach to creating ML systems is to col-

lect a dataset of training examples demonstrating correct
